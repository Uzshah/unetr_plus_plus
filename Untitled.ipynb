{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38b1c52-5d54-4fea-9589-6f075b8c768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unetr_pp.network_architecture.synapse.unetr_pp_synapse import UNETR_PP\n",
    "\n",
    "\n",
    "model = UNETR_PP(in_channels=1,\n",
    "                 out_channels=14,\n",
    "                 img_size=[64, 128, 128],\n",
    "                 feature_size=16,\n",
    "                 num_heads=4,\n",
    "                 depths=[3, 3, 3, 3],\n",
    "                 dims=[32, 64, 128, 256],\n",
    "                 do_ds=True,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc020af-0c6d-4a45-91d7-d8550a76c62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                                  | #parameters or shape   | #flops     |\n",
      "|:----------------------------------------|:-----------------------|:-----------|\n",
      "| model                                   | 42.953M                | 47.936G    |\n",
      "|  unetr_pp_encoder                       |  27.387M               |  12.703G   |\n",
      "|   unetr_pp_encoder.downsample_layers    |   0.346M               |   0.158G   |\n",
      "|    unetr_pp_encoder.downsample_layers.0 |    1.088K              |    38.797M |\n",
      "|    unetr_pp_encoder.downsample_layers.1 |    16.512K             |    68.42M  |\n",
      "|    unetr_pp_encoder.downsample_layers.2 |    65.792K             |    33.882M |\n",
      "|    unetr_pp_encoder.downsample_layers.3 |    0.263M              |    16.859M |\n",
      "|   unetr_pp_encoder.stages               |   27.041M              |   12.545G  |\n",
      "|    unetr_pp_encoder.stages.0            |    9.623M              |    6.943G  |\n",
      "|    unetr_pp_encoder.stages.1            |    2.312M              |    3.258G  |\n",
      "|    unetr_pp_encoder.stages.2            |    3.248M              |    1.576G  |\n",
      "|    unetr_pp_encoder.stages.3            |    11.859M             |    0.768G  |\n",
      "|  encoder1                               |  7.36K                 |  7.919G    |\n",
      "|   encoder1.conv1.conv                   |   0.432K               |   0.453G   |\n",
      "|    encoder1.conv1.conv.weight           |    (16, 1, 3, 3, 3)    |            |\n",
      "|   encoder1.conv2.conv                   |   6.912K               |   7.248G   |\n",
      "|    encoder1.conv2.conv.weight           |    (16, 16, 3, 3, 3)   |            |\n",
      "|   encoder1.conv3.conv                   |   16                   |   16.777M  |\n",
      "|    encoder1.conv3.conv.weight           |    (16, 1, 1, 1, 1)    |            |\n",
      "|   encoder1.norm1                        |                        |   67.109M  |\n",
      "|   encoder1.norm2                        |                        |   67.109M  |\n",
      "|   encoder1.norm3                        |                        |   67.109M  |\n",
      "|  decoder5                               |  3.51M                 |  1.593G    |\n",
      "|   decoder5.transp_conv.conv             |   0.262M               |   16.777M  |\n",
      "|    decoder5.transp_conv.conv.weight     |    (256, 128, 2, 2, 2) |            |\n",
      "|   decoder5.decoder_block.0              |   3.248M               |   1.576G   |\n",
      "|    decoder5.decoder_block.0.0           |    1.083M              |    0.525G  |\n",
      "|    decoder5.decoder_block.0.1           |    1.083M              |    0.525G  |\n",
      "|    decoder5.decoder_block.0.2           |    1.083M              |    0.525G  |\n",
      "|  decoder4                               |  2.378M                |  3.292G    |\n",
      "|   decoder4.transp_conv.conv             |   65.536K              |   33.554M  |\n",
      "|    decoder4.transp_conv.conv.weight     |    (128, 64, 2, 2, 2)  |            |\n",
      "|   decoder4.decoder_block.0              |   2.312M               |   3.258G   |\n",
      "|    decoder4.decoder_block.0.0           |    0.771M              |    1.086G  |\n",
      "|    decoder4.decoder_block.0.1           |    0.771M              |    1.086G  |\n",
      "|    decoder4.decoder_block.0.2           |    0.771M              |    1.086G  |\n",
      "|  decoder3                               |  9.639M                |  7.01G     |\n",
      "|   decoder3.transp_conv.conv             |   16.384K              |   67.109M  |\n",
      "|    decoder3.transp_conv.conv.weight     |    (64, 32, 2, 2, 2)   |            |\n",
      "|   decoder3.decoder_block.0              |   9.623M               |   6.943G   |\n",
      "|    decoder3.decoder_block.0.0           |    3.208M              |    2.314G  |\n",
      "|    decoder3.decoder_block.0.1           |    3.208M              |    2.314G  |\n",
      "|    decoder3.decoder_block.0.2           |    3.208M              |    2.314G  |\n",
      "|  decoder2                               |  30.208K               |  15.167G   |\n",
      "|   decoder2.transp_conv.conv             |   16.384K              |   0.537G   |\n",
      "|    decoder2.transp_conv.conv.weight     |    (32, 16, 2, 4, 4)   |            |\n",
      "|   decoder2.decoder_block.0              |   13.824K              |   14.63G   |\n",
      "|    decoder2.decoder_block.0.conv1.conv  |    6.912K              |    7.248G  |\n",
      "|    decoder2.decoder_block.0.conv2.conv  |    6.912K              |    7.248G  |\n",
      "|    decoder2.decoder_block.0.norm1       |                        |    67.109M |\n",
      "|    decoder2.decoder_block.0.norm2       |                        |    67.109M |\n",
      "|  out1.conv.conv                         |  0.238K                |  0.235G    |\n",
      "|   out1.conv.conv.weight                 |   (14, 16, 1, 1, 1)    |            |\n",
      "|   out1.conv.conv.bias                   |   (14,)                |            |\n",
      "|  out2.conv.conv                         |  0.462K                |  14.68M    |\n",
      "|   out2.conv.conv.weight                 |   (14, 32, 1, 1, 1)    |            |\n",
      "|   out2.conv.conv.bias                   |   (14,)                |            |\n",
      "|  out3.conv.conv                         |  0.91K                 |  3.67M     |\n",
      "|   out3.conv.conv.weight                 |   (14, 64, 1, 1, 1)    |            |\n",
      "|   out3.conv.conv.bias                   |   (14,)                |            |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "\n",
    "input = torch.randn((1, 1, 64, 128, 128))\n",
    "flops = FlopCountAnalysis(model, input)\n",
    "print(flop_count_table(flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7951e5-cb08-44df-b019-cb944a821c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                                  | #parameters or shape   | #flops     |\n",
      "|:----------------------------------------|:-----------------------|:-----------|\n",
      "| model                                   | 19.983M                | 46.223G    |\n",
      "|  unetr_pp_encoder                       |  15.779M               |  11.838G   |\n",
      "|   unetr_pp_encoder.downsample_layers    |   0.346M               |   0.158G   |\n",
      "|    unetr_pp_encoder.downsample_layers.0 |    1.088K              |    38.797M |\n",
      "|    unetr_pp_encoder.downsample_layers.1 |    16.512K             |    68.42M  |\n",
      "|    unetr_pp_encoder.downsample_layers.2 |    65.792K             |    33.882M |\n",
      "|    unetr_pp_encoder.downsample_layers.3 |    0.263M              |    16.859M |\n",
      "|   unetr_pp_encoder.stages               |   15.432M              |   11.68G   |\n",
      "|    unetr_pp_encoder.stages.0            |    0.186M              |    6.37G   |\n",
      "|    unetr_pp_encoder.stages.1            |    0.729M              |    3.052G  |\n",
      "|    unetr_pp_encoder.stages.2            |    2.906M              |    1.507G  |\n",
      "|    unetr_pp_encoder.stages.3            |    11.611M             |    0.751G  |\n",
      "|  encoder1                               |  7.36K                 |  7.919G    |\n",
      "|   encoder1.conv1.conv                   |   0.432K               |   0.453G   |\n",
      "|    encoder1.conv1.conv.weight           |    (16, 1, 3, 3, 3)    |            |\n",
      "|   encoder1.conv2.conv                   |   6.912K               |   7.248G   |\n",
      "|    encoder1.conv2.conv.weight           |    (16, 16, 3, 3, 3)   |            |\n",
      "|   encoder1.conv3.conv                   |   16                   |   16.777M  |\n",
      "|    encoder1.conv3.conv.weight           |    (16, 1, 1, 1, 1)    |            |\n",
      "|   encoder1.norm1                        |                        |   67.109M  |\n",
      "|   encoder1.norm2                        |                        |   67.109M  |\n",
      "|   encoder1.norm3                        |                        |   67.109M  |\n",
      "|  decoder5                               |  3.168M                |  1.524G    |\n",
      "|   decoder5.transp_conv.conv             |   0.262M               |   16.777M  |\n",
      "|    decoder5.transp_conv.conv.weight     |    (256, 128, 2, 2, 2) |            |\n",
      "|   decoder5.decoder_block.0              |   2.906M               |   1.507G   |\n",
      "|    decoder5.decoder_block.0.0           |    0.969M              |    0.502G  |\n",
      "|    decoder5.decoder_block.0.1           |    0.969M              |    0.502G  |\n",
      "|    decoder5.decoder_block.0.2           |    0.969M              |    0.502G  |\n",
      "|  decoder4                               |  0.794M                |  3.086G    |\n",
      "|   decoder4.transp_conv.conv             |   65.536K              |   33.554M  |\n",
      "|    decoder4.transp_conv.conv.weight     |    (128, 64, 2, 2, 2)  |            |\n",
      "|   decoder4.decoder_block.0              |   0.729M               |   3.052G   |\n",
      "|    decoder4.decoder_block.0.0           |    0.243M              |    1.017G  |\n",
      "|    decoder4.decoder_block.0.1           |    0.243M              |    1.017G  |\n",
      "|    decoder4.decoder_block.0.2           |    0.243M              |    1.017G  |\n",
      "|  decoder3                               |  0.203M                |  6.437G    |\n",
      "|   decoder3.transp_conv.conv             |   16.384K              |   67.109M  |\n",
      "|    decoder3.transp_conv.conv.weight     |    (64, 32, 2, 2, 2)   |            |\n",
      "|   decoder3.decoder_block.0              |   0.186M               |   6.37G    |\n",
      "|    decoder3.decoder_block.0.0           |    62.049K             |    2.123G  |\n",
      "|    decoder3.decoder_block.0.1           |    62.049K             |    2.123G  |\n",
      "|    decoder3.decoder_block.0.2           |    62.049K             |    2.123G  |\n",
      "|  decoder2                               |  30.208K               |  15.167G   |\n",
      "|   decoder2.transp_conv.conv             |   16.384K              |   0.537G   |\n",
      "|    decoder2.transp_conv.conv.weight     |    (32, 16, 2, 4, 4)   |            |\n",
      "|   decoder2.decoder_block.0              |   13.824K              |   14.63G   |\n",
      "|    decoder2.decoder_block.0.conv1.conv  |    6.912K              |    7.248G  |\n",
      "|    decoder2.decoder_block.0.conv2.conv  |    6.912K              |    7.248G  |\n",
      "|    decoder2.decoder_block.0.norm1       |                        |    67.109M |\n",
      "|    decoder2.decoder_block.0.norm2       |                        |    67.109M |\n",
      "|  out1.conv.conv                         |  0.238K                |  0.235G    |\n",
      "|   out1.conv.conv.weight                 |   (14, 16, 1, 1, 1)    |            |\n",
      "|   out1.conv.conv.bias                   |   (14,)                |            |\n",
      "|  out2.conv.conv                         |  0.462K                |  14.68M    |\n",
      "|   out2.conv.conv.weight                 |   (14, 32, 1, 1, 1)    |            |\n",
      "|   out2.conv.conv.bias                   |   (14,)                |            |\n",
      "|  out3.conv.conv                         |  0.91K                 |  3.67M     |\n",
      "|   out3.conv.conv.weight                 |   (14, 64, 1, 1, 1)    |            |\n",
      "|   out3.conv.conv.bias                   |   (14,)                |            |\n"
     ]
    }
   ],
   "source": [
    "from unetr_pp.network_architecture.synapse.unetr_pp_synapse import UNETR_PP\n",
    "\n",
    "\n",
    "model = UNETR_PP(in_channels=1,\n",
    "                 out_channels=14,\n",
    "                 img_size=[64, 128, 128],\n",
    "                 feature_size=16,\n",
    "                 num_heads=4,\n",
    "                 depths=[3, 3, 3, 3],\n",
    "                 dims=[32, 64, 128, 256],\n",
    "                 do_ds=True,\n",
    "                 )\n",
    "\n",
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "\n",
    "input = torch.randn((1, 1, 64, 128, 128))\n",
    "flops = FlopCountAnalysis(model, input)\n",
    "print(flop_count_table(flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf3d3bd-c56b-4137-86fa-f72ee85a7cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNETR_PP(\n",
       "  (unetr_pp_encoder): UnetrPPEncoder(\n",
       "    (downsample_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Convolution(\n",
       "          (conv): Conv3d(1, 32, kernel_size=(2, 4, 4), stride=(2, 4, 4), bias=False)\n",
       "        )\n",
       "        (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Convolution(\n",
       "          (conv): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "        )\n",
       "        (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Convolution(\n",
       "          (conv): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "        )\n",
       "        (1): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Convolution(\n",
       "          (conv): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "        )\n",
       "        (1): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (stages): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=32, out_features=64, bias=False)\n",
       "            (semantic_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (geometric_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (semantic_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (geometric_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=32, out_features=64, bias=False)\n",
       "            (semantic_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (geometric_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (semantic_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (geometric_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=32, out_features=64, bias=False)\n",
       "            (semantic_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (geometric_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (semantic_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (geometric_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "            (semantic_query): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (geometric_query): Linear(in_features=16, out_features=16, bias=False)\n",
       "            (semantic_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (geometric_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "            (semantic_query): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (geometric_query): Linear(in_features=16, out_features=16, bias=False)\n",
       "            (semantic_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (geometric_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "            (semantic_query): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (geometric_query): Linear(in_features=16, out_features=16, bias=False)\n",
       "            (semantic_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (geometric_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=128, out_features=256, bias=False)\n",
       "            (semantic_query): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (geometric_query): Linear(in_features=8, out_features=8, bias=False)\n",
       "            (semantic_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (geometric_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=128, out_features=256, bias=False)\n",
       "            (semantic_query): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (geometric_query): Linear(in_features=8, out_features=8, bias=False)\n",
       "            (semantic_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (geometric_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=128, out_features=256, bias=False)\n",
       "            (semantic_query): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (geometric_query): Linear(in_features=8, out_features=8, bias=False)\n",
       "            (semantic_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (geometric_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=256, out_features=512, bias=False)\n",
       "            (semantic_query): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (geometric_query): Linear(in_features=4, out_features=4, bias=False)\n",
       "            (semantic_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (geometric_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=256, out_features=512, bias=False)\n",
       "            (semantic_query): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (geometric_query): Linear(in_features=4, out_features=4, bias=False)\n",
       "            (semantic_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (geometric_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=256, out_features=512, bias=False)\n",
       "            (semantic_query): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (geometric_query): Linear(in_features=4, out_features=4, bias=False)\n",
       "            (semantic_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (geometric_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder1): UnetResBlock(\n",
       "    (conv1): Convolution(\n",
       "      (conv): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (conv2): Convolution(\n",
       "      (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (conv3): Convolution(\n",
       "      (conv): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (norm3): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  )\n",
       "  (decoder5): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (decoder_block): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=128, out_features=256, bias=False)\n",
       "            (semantic_query): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (geometric_query): Linear(in_features=8, out_features=8, bias=False)\n",
       "            (semantic_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (geometric_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=128, out_features=256, bias=False)\n",
       "            (semantic_query): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (geometric_query): Linear(in_features=8, out_features=8, bias=False)\n",
       "            (semantic_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (geometric_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=128, out_features=256, bias=False)\n",
       "            (semantic_query): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (geometric_query): Linear(in_features=8, out_features=8, bias=False)\n",
       "            (semantic_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (geometric_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=128, out_features=64, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder4): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (decoder_block): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "            (semantic_query): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (geometric_query): Linear(in_features=16, out_features=16, bias=False)\n",
       "            (semantic_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (geometric_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "            (semantic_query): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (geometric_query): Linear(in_features=16, out_features=16, bias=False)\n",
       "            (semantic_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (geometric_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=64, out_features=128, bias=False)\n",
       "            (semantic_query): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (geometric_query): Linear(in_features=16, out_features=16, bias=False)\n",
       "            (semantic_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (geometric_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder3): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (decoder_block): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=32, out_features=64, bias=False)\n",
       "            (semantic_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (geometric_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (semantic_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (geometric_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=32, out_features=64, bias=False)\n",
       "            (semantic_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (geometric_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (semantic_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (geometric_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (norm): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          (ecd_block): ECD(\n",
       "            (content_kv): Linear(in_features=32, out_features=64, bias=False)\n",
       "            (semantic_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (geometric_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (semantic_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (semantic_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (geometric_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (geometric_projection): Linear(in_features=32, out_features=16, bias=True)\n",
       "            (semantic_dropout): Dropout(p=0.15, inplace=False)\n",
       "            (geometric_dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "          (conv51): UnetResBlock(\n",
       "            (conv1): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (conv2): Convolution(\n",
       "              (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            )\n",
       "            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "            (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv8): Sequential(\n",
       "            (0): Dropout3d(p=0.1, inplace=False)\n",
       "            (1): Conv3d(32, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder2): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(32, 16, kernel_size=(2, 4, 4), stride=(2, 4, 4), bias=False)\n",
       "    )\n",
       "    (decoder_block): ModuleList(\n",
       "      (0): UnetResBlock(\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        )\n",
       "        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out1): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(16, 14, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       "  (out2): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(32, 14, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       "  (out3): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(64, 14, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ad3f96-2f73-4929-9e80-5e5fe07b178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume shape: (64, 64, 64)\n",
      "Patch size: (8, 8, 8)\n",
      "Number of patches: 512\n",
      "\n",
      "=== FLOP Comparison ===\n",
      "Standard Attention FLOPs: 402,653,184 (0.40B)\n",
      "Local Head FLOPs: 50,331,648 (50.3M)\n",
      "Regional Head FLOPs: 25,165,824 (25.2M)\n",
      "Global Head FLOPs: 786,432 (0.8M)\n",
      "Cross-slice Head FLOPs: 100,663,296 (100.7M)\n",
      "\n",
      "Total Hydra FLOPs: 176,947,200 (176.9M)\n",
      "Speedup: 2.3x\n",
      "Memory reduction: ~2.3x\n",
      "\n",
      "Testing with:\n",
      "Volume shape: (64, 64, 64)\n",
      "Patch size: (8, 8, 8)\n",
      "Number of patches: 512\n",
      "\n",
      "Input shape: torch.Size([2, 512, 768])\n",
      "Output shape: torch.Size([2, 512, 768])\n",
      " Forward pass successful!\n",
      "Output mean: 1.8712\n",
      "Output std: 991.7621\n",
      "Output min: -91737.8672\n",
      "Output max: 91863.3047\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class Efficient3DHydraAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified and robust Hydra Attention for 3D medical image segmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        patch_size: Tuple[int, int, int] = (8, 8, 8),\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Each hydra head gets equal number of attention heads\n",
    "        heads_per_hydra = num_heads // 4\n",
    "        \n",
    "        # Hydra Heads - each specialized for different tasks\n",
    "        self.local_head = LocalBoundaryHead(embed_dim, heads_per_hydra, dropout)\n",
    "        self.regional_head = RegionalContextHead(embed_dim, heads_per_hydra, dropout)\n",
    "        self.global_head = GlobalAnatomyHead(embed_dim, heads_per_hydra, dropout)\n",
    "        self.cross_slice_head = CrossSliceHead(embed_dim, heads_per_hydra, dropout)\n",
    "        \n",
    "        # Adaptive routing - learns which tokens go to which head\n",
    "        self.routing_gate = nn.Linear(embed_dim, 4)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        volume_shape: Tuple[int, int, int],\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [B, N, C] where N = D*H*W patches\n",
    "            volume_shape: Original 3D volume dimensions (D, H, W)\n",
    "            mask: Optional attention mask\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Adaptive routing - decide which tokens need which type of attention\n",
    "        routing_scores = F.softmax(self.routing_gate(x), dim=-1)  # [B, N, 4]\n",
    "        \n",
    "        # Apply each hydra head with adaptive weighting\n",
    "        local_out = self.local_head(x, volume_shape)\n",
    "        regional_out = self.regional_head(x, volume_shape)\n",
    "        global_out = self.global_head(x, volume_shape)\n",
    "        cross_out = self.cross_slice_head(x, volume_shape)\n",
    "        \n",
    "        # Weight and combine outputs\n",
    "        local_weight = routing_scores[:, :, 0:1]\n",
    "        regional_weight = routing_scores[:, :, 1:2]\n",
    "        global_weight = routing_scores[:, :, 2:3]\n",
    "        cross_weight = routing_scores[:, :, 3:4]\n",
    "        \n",
    "        hydra_out = (local_out * local_weight + \n",
    "                    regional_out * regional_weight + \n",
    "                    global_out * global_weight + \n",
    "                    cross_out * cross_weight)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.out_proj(hydra_out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "\n",
    "class LocalBoundaryHead(nn.Module):\n",
    "    \"\"\"Focuses on fine-grained boundaries using windowed attention\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # For simplicity, use local windowed attention with fixed window size\n",
    "        window_size = min(64, N // 8)  # Adaptive window size\n",
    "        \n",
    "        if window_size >= N:\n",
    "            # If sequence is short, use full attention\n",
    "            return self._full_attention(x)\n",
    "        else:\n",
    "            # Use sliding window attention\n",
    "            return self._windowed_attention(x, window_size)\n",
    "    \n",
    "    def _full_attention(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        \n",
    "        return self.proj(self.dropout(out))\n",
    "    \n",
    "    def _windowed_attention(self, x: torch.Tensor, window_size: int) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Pad sequence to be divisible by window_size\n",
    "        pad_len = (window_size - N % window_size) % window_size\n",
    "        if pad_len > 0:\n",
    "            x_padded = F.pad(x, (0, 0, 0, pad_len))\n",
    "        else:\n",
    "            x_padded = x\n",
    "        \n",
    "        N_padded = x_padded.shape[1]\n",
    "        num_windows = N_padded // window_size\n",
    "        \n",
    "        # Reshape to windows\n",
    "        x_windows = x_padded.view(B, num_windows, window_size, C)\n",
    "        x_windows = x_windows.view(-1, window_size, C)  # [B*num_windows, window_size, C]\n",
    "        \n",
    "        # Apply attention within each window\n",
    "        qkv = self.qkv(x_windows).reshape(-1, window_size, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(-1, window_size, C)\n",
    "        \n",
    "        # Reshape back\n",
    "        out = out.view(B, num_windows, window_size, C)\n",
    "        out = out.view(B, N_padded, C)\n",
    "        \n",
    "        # Remove padding\n",
    "        if pad_len > 0:\n",
    "            out = out[:, :N, :]\n",
    "        \n",
    "        return self.proj(self.dropout(out))\n",
    "\n",
    "\n",
    "class RegionalContextHead(nn.Module):\n",
    "    \"\"\"Focuses on medium-range context using strided attention\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Strided sampling - take every 4th token for efficiency\n",
    "        stride = 4\n",
    "        indices = torch.arange(0, N, stride, device=x.device)\n",
    "        x_strided = x[:, indices, :]  # [B, N//4, C]\n",
    "        \n",
    "        # Apply attention on strided tokens\n",
    "        qkv = self.qkv(x_strided).reshape(B, -1, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out_strided = (attn @ v).transpose(1, 2).reshape(B, -1, C)\n",
    "        \n",
    "        # Interpolate back to full resolution\n",
    "        out = torch.zeros_like(x)\n",
    "        out[:, indices, :] = out_strided\n",
    "        \n",
    "        # Simple interpolation for missing positions\n",
    "        for i in range(N):\n",
    "            if i not in indices:\n",
    "                # Find nearest strided positions\n",
    "                left_idx = indices[indices <= i]\n",
    "                right_idx = indices[indices > i]\n",
    "                \n",
    "                if len(left_idx) > 0 and len(right_idx) > 0:\n",
    "                    left = left_idx[-1]\n",
    "                    right = right_idx[0]\n",
    "                    # Linear interpolation\n",
    "                    alpha = (i - left) / (right - left)\n",
    "                    out[:, i, :] = (1 - alpha) * out[:, left, :] + alpha * out[:, right, :]\n",
    "                elif len(left_idx) > 0:\n",
    "                    out[:, i, :] = out[:, left_idx[-1], :]\n",
    "                elif len(right_idx) > 0:\n",
    "                    out[:, i, :] = out[:, right_idx[0], :]\n",
    "        \n",
    "        return self.proj(self.dropout(out))\n",
    "\n",
    "\n",
    "class GlobalAnatomyHead(nn.Module):\n",
    "    \"\"\"Captures global relationships using linear attention\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.to_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.to_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.to_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        q = self.to_q(x)  # [B, N, C]\n",
    "        k = self.to_k(x)  # [B, N, C] \n",
    "        v = self.to_v(x)  # [B, N, C]\n",
    "        \n",
    "        # Linear attention: compute global context then apply to each query\n",
    "        # Global context = sum(k_i * v_i) / sum(k_i)\n",
    "        k_sum = k.sum(dim=1, keepdim=True)  # [B, 1, C]\n",
    "        kv_sum = (k.unsqueeze(-1) * v.unsqueeze(-2)).sum(dim=1)  # [B, C, C]\n",
    "        \n",
    "        # Normalize to avoid division by zero\n",
    "        k_norm = k_sum / (k_sum.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "        \n",
    "        # Apply to each query position\n",
    "        out = torch.zeros_like(q)\n",
    "        for i in range(N):\n",
    "            q_i = q[:, i:i+1, :]  # [B, 1, C]\n",
    "            out[:, i:i+1, :] = q_i @ kv_sum / (q_i @ k_norm.transpose(-2, -1) + 1e-6)\n",
    "        \n",
    "        return self.proj(self.dropout(out))\n",
    "\n",
    "\n",
    "class CrossSliceHead(nn.Module):\n",
    "    \"\"\"Ensures consistency across slice orientations\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Sample a subset of tokens for cross-slice attention\n",
    "        sample_rate = min(1.0, 256 / N)  # Sample up to 256 tokens\n",
    "        num_samples = max(16, int(N * sample_rate))\n",
    "        \n",
    "        # Random sampling for diversity\n",
    "        indices = torch.randperm(N, device=x.device)[:num_samples]\n",
    "        indices = indices.sort()[0]  # Sort to maintain some spatial locality\n",
    "        \n",
    "        x_sampled = x[:, indices, :]  # [B, num_samples, C]\n",
    "        \n",
    "        # Apply attention on sampled tokens\n",
    "        qkv = self.qkv(x_sampled).reshape(B, num_samples, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out_sampled = (attn @ v).transpose(1, 2).reshape(B, num_samples, C)\n",
    "        \n",
    "        # Create global context and broadcast to all positions\n",
    "        global_context = out_sampled.mean(dim=1, keepdim=True)  # [B, 1, C]\n",
    "        out = global_context.expand(B, N, C)\n",
    "        \n",
    "        return self.proj(self.dropout(out))\n",
    "\n",
    "\n",
    "def compare_attention_efficiency():\n",
    "    \"\"\"Compare FLOP counts for different attention mechanisms\"\"\"\n",
    "    \n",
    "    # Typical 3D medical image parameters\n",
    "    batch_size = 2\n",
    "    volume_shape = (64, 64, 64)  # D, H, W\n",
    "    patch_size = (8, 8, 8)\n",
    "    embed_dim = 768\n",
    "    \n",
    "    # Calculate number of patches\n",
    "    num_patches = (volume_shape[0] // patch_size[0]) * \\\n",
    "                  (volume_shape[1] // patch_size[1]) * \\\n",
    "                  (volume_shape[2] // patch_size[2])\n",
    "    \n",
    "    print(f\"Volume shape: {volume_shape}\")\n",
    "    print(f\"Patch size: {patch_size}\")\n",
    "    print(f\"Number of patches: {num_patches}\")\n",
    "    \n",
    "    # FLOP comparison\n",
    "    print(\"\\n=== FLOP Comparison ===\")\n",
    "    \n",
    "    # Standard attention: O(N)\n",
    "    standard_flops = 2 * num_patches * num_patches * embed_dim\n",
    "    print(f\"Standard Attention FLOPs: {standard_flops:,} ({standard_flops/1e9:.2f}B)\")\n",
    "    \n",
    "    # Hydra attention breakdown:\n",
    "    # Local head: windowed attention (window size = 64)\n",
    "    window_size = min(64, num_patches // 8)\n",
    "    num_windows = num_patches // window_size\n",
    "    local_flops = 2 * num_windows * (window_size * window_size) * embed_dim\n",
    "    print(f\"Local Head FLOPs: {local_flops:,} ({local_flops/1e6:.1f}M)\")\n",
    "    \n",
    "    # Regional head: strided attention (stride=4)\n",
    "    strided_patches = num_patches // 4\n",
    "    regional_flops = 2 * strided_patches * strided_patches * embed_dim\n",
    "    print(f\"Regional Head FLOPs: {regional_flops:,} ({regional_flops/1e6:.1f}M)\")\n",
    "    \n",
    "    # Global head: linear attention O(N)\n",
    "    global_flops = 2 * num_patches * embed_dim  # Simplified linear attention\n",
    "    print(f\"Global Head FLOPs: {global_flops:,} ({global_flops/1e6:.1f}M)\")\n",
    "    \n",
    "    # Cross-slice head: sparse sampling\n",
    "    slice_samples = min(256, num_patches)\n",
    "    cross_flops = 2 * slice_samples * slice_samples * embed_dim\n",
    "    print(f\"Cross-slice Head FLOPs: {cross_flops:,} ({cross_flops/1e6:.1f}M)\")\n",
    "    \n",
    "    total_hydra_flops = local_flops + regional_flops + global_flops + cross_flops\n",
    "    print(f\"\\nTotal Hydra FLOPs: {total_hydra_flops:,} ({total_hydra_flops/1e6:.1f}M)\")\n",
    "    print(f\"Speedup: {standard_flops/total_hydra_flops:.1f}x\")\n",
    "    print(f\"Memory reduction: ~{standard_flops/total_hydra_flops:.1f}x\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the implementation\n",
    "    compare_attention_efficiency()\n",
    "    \n",
    "    # Create model\n",
    "    model = Efficient3DHydraAttention(\n",
    "        embed_dim=768,\n",
    "        num_heads=12,\n",
    "        patch_size=(8, 8, 8),\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Test forward pass\n",
    "    batch_size = 2\n",
    "    volume_shape = (64, 64, 64)\n",
    "    patch_size = (8, 8, 8)\n",
    "    \n",
    "    # Calculate correct number of patches\n",
    "    num_patches = (volume_shape[0] // patch_size[0]) * \\\n",
    "                  (volume_shape[1] // patch_size[1]) * \\\n",
    "                  (volume_shape[2] // patch_size[2])\n",
    "    print(f\"\\nTesting with:\")\n",
    "    print(f\"Volume shape: {volume_shape}\")\n",
    "    print(f\"Patch size: {patch_size}\")\n",
    "    print(f\"Number of patches: {num_patches}\")\n",
    "    \n",
    "    x = torch.randn(batch_size, num_patches, 768)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(x, volume_shape)\n",
    "        print(f\"\\nInput shape: {x.shape}\")\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        print(\" Forward pass successful!\")\n",
    "        \n",
    "        # Test that output has correct properties\n",
    "        print(f\"Output mean: {output.mean().item():.4f}\")\n",
    "        print(f\"Output std: {output.std().item():.4f}\")\n",
    "        print(f\"Output min: {output.min().item():.4f}\")\n",
    "        print(f\"Output max: {output.max().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97adbc4-45d8-444d-89b1-a210ab25a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE ATTENTION BENCHMARK: HYDRA vs EPA vs STANDARD\n",
      "================================================================================\n",
      "Device: cuda\n",
      "\n",
      "\n",
      "==================================================\n",
      "TESTING: Small Volume (32)\n",
      "==================================================\n",
      "Volume: (32, 32, 32), Patches: 512, Batch: 4\n",
      "Input tensor: [4, 512, 768]\n",
      "\n",
      "\n",
      "--- Standard Attention ---\n",
      "Parameters: 2,362,368 (2.36M)\n",
      "FLOPs: 6,442,450,944 (6442.5M)\n",
      "Inference Time: 1.95  0.01 ms\n",
      "Peak Memory: 0.156 GB\n",
      "Output Shape: torch.Size([4, 512, 768])\n",
      "\n",
      "--- EPA Attention ---\n",
      "Parameters: 3,015,576 (3.02M)\n",
      "FLOPs: 6,469,189,632 (6469.2M)\n",
      "Inference Time: 2.21  0.00 ms\n",
      "Peak Memory: 0.147 GB\n",
      "Output Shape: torch.Size([4, 512, 768])\n",
      "\n",
      "--- Hydra Attention ---\n",
      "Parameters: 10,043,140 (10.04M)\n",
      "FLOPs: 11,884,560,384 (11884.6M)\n",
      "Inference Time: 48.00  0.15 ms\n",
      "Peak Memory: 0.130 GB\n",
      "Output Shape: torch.Size([4, 512, 768])\n",
      "\n",
      "============================== SUMMARY ==============================\n",
      "Speedup vs Standard:\n",
      "  EPA: 0.88x faster, 1.00x fewer FLOPs, 1.28x params\n",
      "  Hydra: 0.04x faster, 0.54x fewer FLOPs, 4.25x params\n",
      "\n",
      "==================================================\n",
      "TESTING: Medium Volume (64)\n",
      "==================================================\n",
      "Volume: (64, 64, 64), Patches: 512, Batch: 2\n",
      "Input tensor: [2, 512, 768]\n",
      "\n",
      "\n",
      "--- Standard Attention ---\n",
      "Parameters: 2,362,368 (2.36M)\n",
      "FLOPs: 3,221,225,472 (3221.2M)\n",
      "Inference Time: 1.09  0.00 ms\n",
      "Peak Memory: 0.115 GB\n",
      "Output Shape: torch.Size([2, 512, 768])\n",
      "\n",
      "--- EPA Attention ---\n",
      "Parameters: 3,015,576 (3.02M)\n",
      "FLOPs: 3,227,910,144 (3227.9M)\n",
      "Inference Time: 1.39  0.18 ms\n",
      "Peak Memory: 0.108 GB\n",
      "Output Shape: torch.Size([2, 512, 768])\n",
      "\n",
      "--- Hydra Attention ---\n",
      "Parameters: 10,043,140 (10.04M)\n",
      "FLOPs: 5,942,280,192 (5942.3M)\n",
      "Inference Time: 46.01  0.95 ms\n",
      "Peak Memory: 0.099 GB\n",
      "Output Shape: torch.Size([2, 512, 768])\n",
      "\n",
      "============================== SUMMARY ==============================\n",
      "Speedup vs Standard:\n",
      "  EPA: 0.78x faster, 1.00x fewer FLOPs, 1.28x params\n",
      "  Hydra: 0.02x faster, 0.54x fewer FLOPs, 4.25x params\n",
      "\n",
      "==================================================\n",
      "TESTING: Large Volume (128)\n",
      "==================================================\n",
      "Volume: (128, 128, 128), Patches: 512, Batch: 1\n",
      "Input tensor: [1, 512, 768]\n",
      "\n",
      "\n",
      "--- Standard Attention ---\n",
      "Parameters: 2,362,368 (2.36M)\n",
      "FLOPs: 1,610,612,736 (1610.6M)\n",
      "Inference Time: 0.73  0.15 ms\n",
      "Peak Memory: 0.093 GB\n",
      "Output Shape: torch.Size([1, 512, 768])\n",
      "\n",
      "--- EPA Attention ---\n",
      "Parameters: 3,015,576 (3.02M)\n",
      "FLOPs: 1,612,283,904 (1612.3M)\n",
      "Inference Time: 0.71  0.00 ms\n",
      "Peak Memory: 0.089 GB\n",
      "Output Shape: torch.Size([1, 512, 768])\n",
      "\n",
      "--- Hydra Attention ---\n",
      "Parameters: 10,043,140 (10.04M)\n",
      "FLOPs: 2,971,140,096 (2971.1M)\n",
      "Inference Time: 44.29  0.13 ms\n",
      "Peak Memory: 0.085 GB\n",
      "Output Shape: torch.Size([1, 512, 768])\n",
      "\n",
      "============================== SUMMARY ==============================\n",
      "Speedup vs Standard:\n",
      "  EPA: 1.02x faster, 1.00x fewer FLOPs, 1.28x params\n",
      "  Hydra: 0.02x faster, 0.54x fewer FLOPs, 4.25x params\n",
      "\n",
      "================================================================================\n",
      "FINAL COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Small Volume (32):\n",
      "  Hydra vs Standard: 0.0x faster, 0.5x fewer FLOPs\n",
      "  EPA vs Hydra: 21.7x faster\n",
      "    FLOP ratio: 1.8x\n",
      "\n",
      "Medium Volume (64):\n",
      "  Hydra vs Standard: 0.0x faster, 0.5x fewer FLOPs\n",
      "  EPA vs Hydra: 33.1x faster\n",
      "    FLOP ratio: 1.8x\n",
      "\n",
      "Large Volume (128):\n",
      "  Hydra vs Standard: 0.0x faster, 0.5x fewer FLOPs\n",
      "  EPA vs Hydra: 62.1x faster\n",
      "    FLOP ratio: 1.8x\n",
      "\n",
      "================================================================================\n",
      "BENCHMARK COMPLETED!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "# EPA Implementation (from UNETR++)\n",
    "class EPA(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient Paired Attention Block, based on: \"Shaker et al.,\n",
    "    UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation\"\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, proj_size, num_heads=4, qkv_bias=False,\n",
    "                 channel_attn_drop=0.1, spatial_attn_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.temperature2 = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        # qkvv are 4 linear layers (query_shared, key_shared, value_spatial, value_channel)\n",
    "        self.qkvv = nn.Linear(hidden_size, hidden_size * 4, bias=qkv_bias)\n",
    "        # E and F are projection matrices with shared weights used in spatial attention module to project\n",
    "        # keys and values from HWD-dimension to P-dimension\n",
    "        self.E = self.F = nn.Linear(input_size, proj_size)\n",
    "        self.attn_drop = nn.Dropout(channel_attn_drop)\n",
    "        self.attn_drop_2 = nn.Dropout(spatial_attn_drop)\n",
    "        self.out_proj = nn.Linear(hidden_size, int(hidden_size // 2))\n",
    "        self.out_proj2 = nn.Linear(hidden_size, int(hidden_size // 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkvv = self.qkvv(x).reshape(B, N, 4, self.num_heads, C // self.num_heads)\n",
    "        qkvv = qkvv.permute(2, 0, 3, 1, 4)\n",
    "        q_shared, k_shared, v_CA, v_SA = qkvv[0], qkvv[1], qkvv[2], qkvv[3]\n",
    "        q_shared = q_shared.transpose(-2, -1)\n",
    "        k_shared = k_shared.transpose(-2, -1)\n",
    "        v_CA = v_CA.transpose(-2, -1)\n",
    "        v_SA = v_SA.transpose(-2, -1)\n",
    "        k_shared_projected = self.E(k_shared)\n",
    "        v_SA_projected = self.F(v_SA)\n",
    "        q_shared = torch.nn.functional.normalize(q_shared, dim=-1)\n",
    "        k_shared = torch.nn.functional.normalize(k_shared, dim=-1)\n",
    "        attn_CA = (q_shared @ k_shared.transpose(-2, -1)) * self.temperature\n",
    "        attn_CA = attn_CA.softmax(dim=-1)\n",
    "        attn_CA = self.attn_drop(attn_CA)\n",
    "        x_CA = (attn_CA @ v_CA).permute(0, 3, 1, 2).reshape(B, N, C)\n",
    "        attn_SA = (q_shared.permute(0, 1, 3, 2) @ k_shared_projected) * self.temperature2\n",
    "        attn_SA = attn_SA.softmax(dim=-1)\n",
    "        attn_SA = self.attn_drop_2(attn_SA)\n",
    "        x_SA = (attn_SA @ v_SA_projected.transpose(-2, -1)).permute(0, 3, 1, 2).reshape(B, N, C)\n",
    "        # Concat fusion\n",
    "        x_SA = self.out_proj(x_SA)\n",
    "        x_CA = self.out_proj2(x_CA)\n",
    "        x = torch.cat((x_SA, x_CA), dim=-1)\n",
    "        return x\n",
    "    \n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'temperature', 'temperature2'}\n",
    "\n",
    "\n",
    "# Simplified Hydra Attention (from previous implementation)\n",
    "class Efficient3DHydraAttention(nn.Module):\n",
    "    \"\"\"Hydra Attention for 3D medical image segmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int = 768, num_heads: int = 12, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Each hydra head gets equal number of attention heads\n",
    "        heads_per_hydra = num_heads // 4\n",
    "        \n",
    "        # Hydra Heads - each specialized for different tasks\n",
    "        self.local_head = LocalBoundaryHead(embed_dim, heads_per_hydra, dropout)\n",
    "        self.regional_head = RegionalContextHead(embed_dim, heads_per_hydra, dropout)\n",
    "        self.global_head = GlobalAnatomyHead(embed_dim, heads_per_hydra, dropout)\n",
    "        self.cross_slice_head = CrossSliceHead(embed_dim, heads_per_hydra, dropout)\n",
    "        \n",
    "        # Adaptive routing - learns which tokens go to which head\n",
    "        self.routing_gate = nn.Linear(embed_dim, 4)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int] = (64, 64, 64)) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Adaptive routing\n",
    "        routing_scores = F.softmax(self.routing_gate(x), dim=-1)\n",
    "        \n",
    "        # Apply each hydra head with adaptive weighting\n",
    "        local_out = self.local_head(x, volume_shape)\n",
    "        regional_out = self.regional_head(x, volume_shape)\n",
    "        global_out = self.global_head(x, volume_shape)\n",
    "        cross_out = self.cross_slice_head(x, volume_shape)\n",
    "        \n",
    "        # Weight and combine outputs\n",
    "        local_weight = routing_scores[:, :, 0:1]\n",
    "        regional_weight = routing_scores[:, :, 1:2]\n",
    "        global_weight = routing_scores[:, :, 2:3]\n",
    "        cross_weight = routing_scores[:, :, 3:4]\n",
    "        \n",
    "        hydra_out = (local_out * local_weight + \n",
    "                    regional_out * regional_weight + \n",
    "                    global_out * global_weight + \n",
    "                    cross_out * cross_weight)\n",
    "        \n",
    "        # Final projection\n",
    "        out = self.out_proj(hydra_out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "\n",
    "# Hydra Head implementations (simplified for benchmarking)\n",
    "class LocalBoundaryHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        window_size = min(64, N // 8)\n",
    "        \n",
    "        if window_size >= N:\n",
    "            return self._full_attention(x)\n",
    "        else:\n",
    "            return self._windowed_attention(x, window_size)\n",
    "    \n",
    "    def _full_attention(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        \n",
    "        return self.proj(out)\n",
    "    \n",
    "    def _windowed_attention(self, x: torch.Tensor, window_size: int) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Simplified windowed attention\n",
    "        pad_len = (window_size - N % window_size) % window_size\n",
    "        if pad_len > 0:\n",
    "            x_padded = F.pad(x, (0, 0, 0, pad_len))\n",
    "        else:\n",
    "            x_padded = x\n",
    "        \n",
    "        N_padded = x_padded.shape[1]\n",
    "        num_windows = N_padded // window_size\n",
    "        \n",
    "        x_windows = x_padded.view(B, num_windows, window_size, C)\n",
    "        x_windows = x_windows.view(-1, window_size, C)\n",
    "        \n",
    "        qkv = self.qkv(x_windows).reshape(-1, window_size, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(-1, window_size, C)\n",
    "        \n",
    "        out = out.view(B, N_padded, C)\n",
    "        if pad_len > 0:\n",
    "            out = out[:, :N, :]\n",
    "        \n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class RegionalContextHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Strided sampling\n",
    "        stride = 4\n",
    "        indices = torch.arange(0, N, stride, device=x.device)\n",
    "        x_strided = x[:, indices, :]\n",
    "        \n",
    "        qkv = self.qkv(x_strided).reshape(B, -1, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out_strided = (attn @ v).transpose(1, 2).reshape(B, -1, C)\n",
    "        \n",
    "        # Simple interpolation back\n",
    "        out = torch.zeros_like(x)\n",
    "        out[:, indices, :] = out_strided\n",
    "        \n",
    "        # Fill missing positions with nearest neighbor\n",
    "        for i in range(N):\n",
    "            if i not in indices:\n",
    "                nearest = indices[torch.argmin(torch.abs(indices - i))]\n",
    "                out[:, i, :] = out[:, nearest, :]\n",
    "        \n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class GlobalAnatomyHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.to_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.to_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.to_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "        \n",
    "        # Linear attention approximation\n",
    "        k_sum = k.sum(dim=1, keepdim=True)\n",
    "        kv = (k.transpose(-2, -1) @ v) / N\n",
    "        out = q @ kv\n",
    "        \n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class CrossSliceHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Sample subset for efficiency\n",
    "        sample_rate = min(1.0, 256 / N)\n",
    "        num_samples = max(16, int(N * sample_rate))\n",
    "        \n",
    "        indices = torch.randperm(N, device=x.device)[:num_samples]\n",
    "        x_sampled = x[:, indices, :]\n",
    "        \n",
    "        qkv = self.qkv(x_sampled).reshape(B, num_samples, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out_sampled = (attn @ v).transpose(1, 2).reshape(B, num_samples, C)\n",
    "        \n",
    "        # Broadcast global context\n",
    "        global_context = out_sampled.mean(dim=1, keepdim=True)\n",
    "        out = global_context.expand(B, N, C)\n",
    "        \n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "# Standard Multi-Head Attention for comparison\n",
    "class StandardAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int] = (64, 64, 64)) -> torch.Tensor:\n",
    "        out, _ = self.attention(x, x, x)\n",
    "        return out\n",
    "\n",
    "\n",
    "def count_flops(model, x, volume_shape=(64, 64, 64)):\n",
    "    \"\"\"Count FLOPs for attention mechanisms\"\"\"\n",
    "    B, N, C = x.shape\n",
    "    \n",
    "    if isinstance(model, EPA):\n",
    "        # EPA FLOP calculation\n",
    "        # qkvv projection: 4 * N * C * C\n",
    "        qkvv_flops = 4 * N * C * C\n",
    "        \n",
    "        # E and F projections: 2 * num_heads * C/num_heads * N * proj_size\n",
    "        proj_size = getattr(model, 'E').out_features\n",
    "        input_size = getattr(model, 'E').in_features\n",
    "        ef_flops = 2 * model.num_heads * (C // model.num_heads) * input_size * proj_size\n",
    "        \n",
    "        # Channel attention: Q @ K^T = (C/num_heads)^2 * num_heads\n",
    "        ca_attn_flops = model.num_heads * (C // model.num_heads) * (C // model.num_heads) * B\n",
    "        # Channel attention: Attn @ V = (C/num_heads)^2 * num_heads  \n",
    "        ca_out_flops = model.num_heads * (C // model.num_heads) * (C // model.num_heads) * B\n",
    "        \n",
    "        # Spatial attention: Q @ K_proj = N * proj_size * num_heads\n",
    "        sa_attn_flops = model.num_heads * N * proj_size * B\n",
    "        # Spatial attention: Attn @ V_proj = N * proj_size * num_heads\n",
    "        sa_out_flops = model.num_heads * N * proj_size * B\n",
    "        \n",
    "        # Output projections: 2 * N * C * C/2\n",
    "        out_proj_flops = 2 * N * C * (C // 2)\n",
    "        \n",
    "        total_flops = (qkvv_flops + ef_flops + ca_attn_flops + ca_out_flops + \n",
    "                      sa_attn_flops + sa_out_flops + out_proj_flops) * B\n",
    "        \n",
    "    elif isinstance(model, StandardAttention):\n",
    "        # Standard attention: O(N^2 * C)\n",
    "        # Q@K^T: N^2 * C, Attn@V: N^2 * C, plus linear projections: 3 * N * C^2\n",
    "        total_flops = B * (2 * N * N * C + 4 * N * C * C)  # QKV + attention + output proj\n",
    "        \n",
    "    elif isinstance(model, Efficient3DHydraAttention):\n",
    "        # Hydra attention approximation\n",
    "        # Local head: windowed attention\n",
    "        window_size = min(64, N // 8)\n",
    "        num_windows = N // window_size if window_size < N else 1\n",
    "        local_flops = num_windows * (2 * window_size * window_size * C + 3 * window_size * C * C)\n",
    "        \n",
    "        # Regional head: strided attention  \n",
    "        strided_N = N // 4\n",
    "        regional_flops = 2 * strided_N * strided_N * C + 3 * strided_N * C * C\n",
    "        \n",
    "        # Global head: linear attention O(N * C^2)\n",
    "        global_flops = 3 * N * C * C\n",
    "        \n",
    "        # Cross-slice head: sparse sampling\n",
    "        sample_N = min(256, N)\n",
    "        cross_flops = 2 * sample_N * sample_N * C + 3 * sample_N * C * C\n",
    "        \n",
    "        # Routing and projections\n",
    "        routing_flops = N * C * 4 + N * C * C\n",
    "        \n",
    "        total_flops = B * (local_flops + regional_flops + global_flops + cross_flops + routing_flops)\n",
    "        \n",
    "    return total_flops\n",
    "\n",
    "\n",
    "def benchmark_models():\n",
    "    \"\"\"Comprehensive benchmark of different attention mechanisms\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE ATTENTION BENCHMARK: HYDRA vs EPA vs STANDARD\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Test configurations for 3D medical imaging\n",
    "    configs = [\n",
    "        {\"name\": \"Small Volume (32)\", \"batch_size\": 4, \"volume_shape\": (32, 32, 32), \"patch_size\": (4, 4, 4)},\n",
    "        {\"name\": \"Medium Volume (64)\", \"batch_size\": 2, \"volume_shape\": (64, 64, 64), \"patch_size\": (8, 8, 8)},\n",
    "        {\"name\": \"Large Volume (128)\", \"batch_size\": 1, \"volume_shape\": (128, 128, 128), \"patch_size\": (16, 16, 16)},\n",
    "    ]\n",
    "    \n",
    "    embed_dim = 768\n",
    "    num_heads = 12\n",
    "    dropout = 0.1\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"TESTING: {config['name']}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        batch_size = config[\"batch_size\"]\n",
    "        volume_shape = config[\"volume_shape\"]\n",
    "        patch_size = config[\"patch_size\"]\n",
    "        \n",
    "        # Calculate number of patches\n",
    "        num_patches = np.prod([v // p for v, p in zip(volume_shape, patch_size)])\n",
    "        input_size = num_patches  # For EPA\n",
    "        proj_size = min(256, num_patches // 4)  # Projection size for EPA\n",
    "        \n",
    "        print(f\"Volume: {volume_shape}, Patches: {num_patches}, Batch: {batch_size}\")\n",
    "        print(f\"Input tensor: [{batch_size}, {num_patches}, {embed_dim}]\")\n",
    "        print()\n",
    "        \n",
    "        # Create models\n",
    "        models = {\n",
    "            \"Standard\": StandardAttention(embed_dim, num_heads, dropout).to(device),\n",
    "            \"EPA\": EPA(input_size, embed_dim, proj_size, num_heads).to(device),\n",
    "            \"Hydra\": Efficient3DHydraAttention(embed_dim, num_heads, dropout).to(device)\n",
    "        }\n",
    "        \n",
    "        # Create input\n",
    "        x = torch.randn(batch_size, num_patches, embed_dim, device=device)\n",
    "        \n",
    "        # Benchmark each model\n",
    "        config_results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\n--- {name} Attention ---\")\n",
    "            \n",
    "            # Parameter count\n",
    "            param_count = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"Parameters: {param_count:,} ({param_count/1e6:.2f}M)\")\n",
    "            \n",
    "            # FLOP count\n",
    "            flops = count_flops(model, x, volume_shape)\n",
    "            print(f\"FLOPs: {flops:,} ({flops/1e6:.1f}M)\")\n",
    "            \n",
    "            # Memory usage\n",
    "            model.train()\n",
    "            torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
    "            \n",
    "            try:\n",
    "                # Warmup\n",
    "                with torch.no_grad():\n",
    "                    if name == \"Hydra\":\n",
    "                        _ = model(x, volume_shape)\n",
    "                    else:\n",
    "                        _ = model(x)\n",
    "                \n",
    "                # Memory measurement\n",
    "                if device.type == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "                    torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                # Forward pass timing\n",
    "                model.eval()\n",
    "                times = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for _ in range(10):  # Multiple runs for stable timing\n",
    "                        torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        if name == \"Hydra\":\n",
    "                            output = model(x, volume_shape)\n",
    "                        else:\n",
    "                            output = model(x)\n",
    "                        \n",
    "                        torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "                        end_time = time.time()\n",
    "                        \n",
    "                        times.append(end_time - start_time)\n",
    "                \n",
    "                avg_time = np.mean(times[2:])  # Exclude first 2 for warmup\n",
    "                std_time = np.std(times[2:])\n",
    "                \n",
    "                # Peak memory\n",
    "                if device.type == \"cuda\":\n",
    "                    peak_memory = torch.cuda.max_memory_allocated() / (1024**3)  # GB\n",
    "                else:\n",
    "                    peak_memory = 0\n",
    "                \n",
    "                print(f\"Inference Time: {avg_time*1000:.2f}  {std_time*1000:.2f} ms\")\n",
    "                print(f\"Peak Memory: {peak_memory:.3f} GB\")\n",
    "                print(f\"Output Shape: {output.shape}\")\n",
    "                \n",
    "                # Store results\n",
    "                config_results[name] = {\n",
    "                    \"params\": param_count,\n",
    "                    \"flops\": flops,\n",
    "                    \"time_ms\": avg_time * 1000,\n",
    "                    \"time_std\": std_time * 1000,\n",
    "                    \"memory_gb\": peak_memory,\n",
    "                    \"throughput\": batch_size / avg_time\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Failed: {e}\")\n",
    "                config_results[name] = None\n",
    "        \n",
    "        results[config['name']] = config_results\n",
    "        \n",
    "        # Comparison summary for this configuration\n",
    "        print(f\"\\n{'='*30} SUMMARY {'='*30}\")\n",
    "        valid_results = {k: v for k, v in config_results.items() if v is not None}\n",
    "        \n",
    "        if len(valid_results) > 1:\n",
    "            baseline = \"Standard\"\n",
    "            if baseline in valid_results:\n",
    "                print(f\"Speedup vs {baseline}:\")\n",
    "                for name, result in valid_results.items():\n",
    "                    if name != baseline:\n",
    "                        speedup = valid_results[baseline][\"time_ms\"] / result[\"time_ms\"]\n",
    "                        flop_reduction = valid_results[baseline][\"flops\"] / result[\"flops\"]\n",
    "                        param_ratio = result[\"params\"] / valid_results[baseline][\"params\"]\n",
    "                        print(f\"  {name}: {speedup:.2f}x faster, {flop_reduction:.2f}x fewer FLOPs, {param_ratio:.2f}x params\")\n",
    "    \n",
    "    # Overall summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for config_name, config_results in results.items():\n",
    "        print(f\"\\n{config_name}:\")\n",
    "        valid_results = {k: v for k, v in config_results.items() if v is not None}\n",
    "        \n",
    "        if \"Standard\" in valid_results and \"Hydra\" in valid_results:\n",
    "            hydra = valid_results[\"Hydra\"]\n",
    "            standard = valid_results[\"Standard\"]\n",
    "            speedup = standard[\"time_ms\"] / hydra[\"time_ms\"]\n",
    "            flop_reduction = standard[\"flops\"] / hydra[\"flops\"]\n",
    "            print(f\"  Hydra vs Standard: {speedup:.1f}x faster, {flop_reduction:.1f}x fewer FLOPs\")\n",
    "        \n",
    "        if \"EPA\" in valid_results and \"Hydra\" in valid_results:\n",
    "            hydra = valid_results[\"Hydra\"]\n",
    "            epa = valid_results[\"EPA\"]\n",
    "            speedup = epa[\"time_ms\"] / hydra[\"time_ms\"]\n",
    "            flop_ratio = hydra[\"flops\"] / epa[\"flops\"]\n",
    "            print(f\"  Hydra vs EPA: {speedup:.1f}x faster\" if speedup > 1 else f\"  EPA vs Hydra: {1/speedup:.1f}x faster\")\n",
    "            print(f\"    FLOP ratio: {flop_ratio:.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Run comprehensive benchmark\n",
    "    benchmark_results = benchmark_models()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BENCHMARK COMPLETED!\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "681e46d2-afb1-4766-977b-8a1a8978b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OPTIMIZED HYDRA BENCHMARK\n",
      "================================================================================\n",
      "\n",
      "--- Standard ---\n",
      "Parameters: 2,362,368 (2.36M)\n",
      "Time: 1.22  0.26 ms\n",
      "Output shape: torch.Size([2, 512, 768])\n",
      "Peak Memory: 0.079 GB\n",
      "\n",
      "--- Optimized Hydra ---\n",
      "Parameters: 2,368,512 (2.37M)\n",
      "Time: 1.34  0.11 ms\n",
      "Output shape: torch.Size([2, 512, 768])\n",
      "Peak Memory: 0.065 GB\n",
      "\n",
      "--- Ultra-Fast Hydra ---\n",
      "Parameters: 2,359,296 (2.36M)\n",
      "Time: 51.31  1.56 ms\n",
      "Output shape: torch.Size([2, 512, 768])\n",
      "Peak Memory: 0.057 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class OptimizedHydraAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Highly optimized Hydra Attention designed to outperform both Standard and EPA attention.\n",
    "    Key optimizations:\n",
    "    - Parallel processing of all heads\n",
    "    - Fused operations to reduce memory overhead\n",
    "    - Efficient sparse patterns\n",
    "    - Minimal parameter overhead\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        dropout: float = 0.1,\n",
    "        efficiency_mode: str = \"balanced\"  # \"fast\", \"balanced\", \"memory\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.efficiency_mode = efficiency_mode\n",
    "        \n",
    "        # Single unified QKV projection for all heads (more efficient than separate projections)\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        \n",
    "        # Lightweight routing mechanism (much simpler than before)\n",
    "        self.route_proj = nn.Linear(embed_dim, num_heads, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Precompute attention patterns for efficiency\n",
    "        self._register_attention_patterns()\n",
    "        \n",
    "    def _register_attention_patterns(self):\n",
    "        \"\"\"Precompute sparse attention patterns to avoid runtime computation\"\"\"\n",
    "        # These will be populated during first forward pass based on sequence length\n",
    "        self.register_buffer('local_mask', None, persistent=False)\n",
    "        self.register_buffer('strided_mask', None, persistent=False)\n",
    "        self.register_buffer('global_indices', None, persistent=False)\n",
    "        \n",
    "    def _create_attention_masks(self, seq_len: int, device: torch.device):\n",
    "        \"\"\"Create efficient sparse attention masks\"\"\"\n",
    "        if self.local_mask is not None and self.local_mask.size(0) == seq_len:\n",
    "            return  # Already created for this sequence length\n",
    "            \n",
    "        # Local attention pattern (sliding window)\n",
    "        window_size = min(64, seq_len // 4)\n",
    "        local_mask = torch.zeros(seq_len, seq_len, device=device, dtype=torch.bool)\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - window_size // 2)\n",
    "            end = min(seq_len, i + window_size // 2 + 1)\n",
    "            local_mask[i, start:end] = True\n",
    "            \n",
    "        # Strided attention pattern\n",
    "        stride = max(1, seq_len // 128)  # Adaptive stride\n",
    "        strided_mask = torch.zeros(seq_len, seq_len, device=device, dtype=torch.bool)\n",
    "        for i in range(0, seq_len, stride):\n",
    "            for j in range(0, seq_len, stride):\n",
    "                strided_mask[i, j] = True\n",
    "                \n",
    "        # Global attention indices (sample key positions)\n",
    "        global_indices = torch.linspace(0, seq_len-1, min(32, seq_len), device=device).long()\n",
    "        \n",
    "        # Register as buffers\n",
    "        self.register_buffer('local_mask', local_mask, persistent=False)\n",
    "        self.register_buffer('strided_mask', strided_mask, persistent=False)\n",
    "        self.register_buffer('global_indices', global_indices, persistent=False)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int] = (64, 64, 64)) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Create attention masks if needed\n",
    "        self._create_attention_masks(N, x.device)\n",
    "        \n",
    "        # Unified QKV projection (more efficient than separate projections)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, num_heads, N, head_dim]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Lightweight routing - which heads use which patterns\n",
    "        route_weights = torch.softmax(self.route_proj(x.mean(dim=1)), dim=-1)  # [B, num_heads]\n",
    "        \n",
    "        # Parallel attention computation with different patterns per head\n",
    "        attn_out = torch.zeros_like(q)  # [B, num_heads, N, head_dim]\n",
    "        \n",
    "        # Distribute heads across different attention patterns\n",
    "        heads_per_pattern = self.num_heads // 4\n",
    "        \n",
    "        # Pattern 1: Local attention (fine details)\n",
    "        local_heads = slice(0, heads_per_pattern)\n",
    "        attn_out[:, local_heads] = self._local_attention(\n",
    "            q[:, local_heads], k[:, local_heads], v[:, local_heads]\n",
    "        )\n",
    "        \n",
    "        # Pattern 2: Strided attention (medium range)  \n",
    "        strided_heads = slice(heads_per_pattern, 2 * heads_per_pattern)\n",
    "        attn_out[:, strided_heads] = self._strided_attention(\n",
    "            q[:, strided_heads], k[:, strided_heads], v[:, strided_heads]\n",
    "        )\n",
    "        \n",
    "        # Pattern 3: Global attention (long range) - Linear attention for efficiency\n",
    "        global_heads = slice(2 * heads_per_pattern, 3 * heads_per_pattern)\n",
    "        attn_out[:, global_heads] = self._global_linear_attention(\n",
    "            q[:, global_heads], k[:, global_heads], v[:, global_heads]\n",
    "        )\n",
    "        \n",
    "        # Pattern 4: Random sparse attention (cross-slice consistency)\n",
    "        sparse_heads = slice(3 * heads_per_pattern, self.num_heads)\n",
    "        attn_out[:, sparse_heads] = self._sparse_attention(\n",
    "            q[:, sparse_heads], k[:, sparse_heads], v[:, sparse_heads]\n",
    "        )\n",
    "        \n",
    "        # Apply routing weights\n",
    "        route_weights = route_weights.unsqueeze(-1).unsqueeze(-1)  # [B, num_heads, 1, 1]\n",
    "        attn_out = attn_out * route_weights\n",
    "        \n",
    "        # Combine heads and project\n",
    "        out = attn_out.transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        return self.dropout(out)\n",
    "    \n",
    "    def _local_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Efficient local attention using sparse mask\"\"\"\n",
    "        B, H, N, D = q.shape\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, H, N, N]\n",
    "        \n",
    "        # Apply local mask (only attend to nearby tokens)\n",
    "        attn = attn.masked_fill(~self.local_mask.unsqueeze(0).unsqueeze(0), -float('inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        out = attn @ v  # [B, H, N, D]\n",
    "        return out\n",
    "    \n",
    "    def _strided_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Efficient strided attention\"\"\"\n",
    "        B, H, N, D = q.shape\n",
    "        \n",
    "        # Use precomputed strided mask\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.masked_fill(~self.strided_mask.unsqueeze(0).unsqueeze(0), -float('inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = attn @ v\n",
    "        return out\n",
    "    \n",
    "    def _global_linear_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Linear attention for global context - O(N) complexity\"\"\"\n",
    "        B, H, N, D = q.shape\n",
    "        \n",
    "        # Normalize queries and keys for stability\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "        \n",
    "        # Linear attention: compute k^T @ v first (more efficient)\n",
    "        kv = k.transpose(-2, -1) @ v  # [B, H, D, D]\n",
    "        out = q @ kv  # [B, H, N, D]\n",
    "        \n",
    "        # Normalize by key sum to maintain attention property\n",
    "        k_sum = k.sum(dim=-2, keepdim=True)  # [B, H, 1, D]\n",
    "        normalizer = (q * k_sum).sum(dim=-1, keepdim=True) + 1e-6  # [B, H, N, 1]\n",
    "        out = out / normalizer\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _sparse_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Sparse random attention for cross-connections\"\"\"\n",
    "        B, H, N, D = q.shape\n",
    "        \n",
    "        # Sample a subset of key positions for each query\n",
    "        if self.efficiency_mode == \"fast\":\n",
    "            # Very sparse for maximum speed\n",
    "            sample_ratio = min(0.1, 32 / N)\n",
    "        elif self.efficiency_mode == \"balanced\":\n",
    "            sample_ratio = min(0.25, 64 / N)\n",
    "        else:  # memory mode\n",
    "            sample_ratio = min(0.5, 128 / N)\n",
    "            \n",
    "        num_samples = max(4, int(N * sample_ratio))\n",
    "        \n",
    "        # Use global indices for consistent sampling\n",
    "        indices = self.global_indices[:num_samples]\n",
    "        \n",
    "        # Sample keys and values\n",
    "        k_sampled = k[:, :, indices, :]  # [B, H, num_samples, D]\n",
    "        v_sampled = v[:, :, indices, :]  # [B, H, num_samples, D]\n",
    "        \n",
    "        # Compute attention with sampled keys\n",
    "        attn = (q @ k_sampled.transpose(-2, -1)) * self.scale  # [B, H, N, num_samples]\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        # Apply to sampled values\n",
    "        out = attn @ v_sampled  # [B, H, N, D]\n",
    "        return out\n",
    "\n",
    "\n",
    "class UltraFastHydra(nn.Module):\n",
    "    \"\"\"\n",
    "    Ultra-optimized version focusing purely on speed\n",
    "    Trades some accuracy for maximum performance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int = 768, num_heads: int = 12, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Single QKV projection with minimal overhead\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # Fixed patterns - no learnable routing for maximum speed\n",
    "        self.window_size = 32  # Fixed small window\n",
    "        self.stride = 8        # Fixed stride\n",
    "        self.global_tokens = 16  # Fixed number of global tokens\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, volume_shape: Tuple[int, int, int] = (64, 64, 64)) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Single QKV computation\n",
    "        qkv = self.qkv(x).view(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Split heads evenly across patterns\n",
    "        heads_per_pattern = self.num_heads // 3\n",
    "        \n",
    "        out = torch.zeros_like(q)\n",
    "        \n",
    "        # Pattern 1: Local sliding window (fast)\n",
    "        if heads_per_pattern > 0:\n",
    "            local_slice = slice(0, heads_per_pattern)\n",
    "            out[:, local_slice] = self._fast_local(q[:, local_slice], k[:, local_slice], v[:, local_slice], N)\n",
    "        \n",
    "        # Pattern 2: Strided attention (medium range)\n",
    "        if heads_per_pattern > 0:\n",
    "            strided_slice = slice(heads_per_pattern, 2 * heads_per_pattern)\n",
    "            out[:, strided_slice] = self._fast_strided(q[:, strided_slice], k[:, strided_slice], v[:, strided_slice], N)\n",
    "        \n",
    "        # Pattern 3: Linear attention (global)\n",
    "        remaining_heads = self.num_heads - 2 * heads_per_pattern\n",
    "        if remaining_heads > 0:\n",
    "            global_slice = slice(2 * heads_per_pattern, self.num_heads)\n",
    "            out[:, global_slice] = self._fast_linear(q[:, global_slice], k[:, global_slice], v[:, global_slice])\n",
    "        \n",
    "        # Combine and project\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "    def _fast_local(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, N: int) -> torch.Tensor:\n",
    "        \"\"\"Extremely fast local attention with fixed window\"\"\"\n",
    "        B, H, _, D = q.shape\n",
    "        \n",
    "        # Pad sequence to be divisible by window_size\n",
    "        pad_size = (self.window_size - N % self.window_size) % self.window_size\n",
    "        if pad_size > 0:\n",
    "            q = F.pad(q, (0, 0, 0, pad_size))\n",
    "            k = F.pad(k, (0, 0, 0, pad_size))\n",
    "            v = F.pad(v, (0, 0, 0, pad_size))\n",
    "        \n",
    "        N_padded = q.size(2)\n",
    "        num_windows = N_padded // self.window_size\n",
    "        \n",
    "        # Reshape to windows\n",
    "        q = q.view(B, H, num_windows, self.window_size, D)\n",
    "        k = k.view(B, H, num_windows, self.window_size, D)\n",
    "        v = v.view(B, H, num_windows, self.window_size, D)\n",
    "        \n",
    "        # Attention within each window\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = attn @ v\n",
    "        \n",
    "        # Reshape back\n",
    "        out = out.view(B, H, N_padded, D)\n",
    "        \n",
    "        # Remove padding\n",
    "        if pad_size > 0:\n",
    "            out = out[:, :, :N, :]\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def _fast_strided(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, N: int) -> torch.Tensor:\n",
    "        \"\"\"Fast strided attention\"\"\"\n",
    "        # Sample every stride-th token\n",
    "        indices = torch.arange(0, N, self.stride, device=q.device)\n",
    "        \n",
    "        q_strided = q[:, :, indices, :]\n",
    "        k_strided = k[:, :, indices, :]\n",
    "        v_strided = v[:, :, indices, :]\n",
    "        \n",
    "        # Attention on strided tokens\n",
    "        attn = (q_strided @ k_strided.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out_strided = attn @ v_strided\n",
    "        \n",
    "        # Interpolate back to full resolution\n",
    "        out = torch.zeros_like(q)\n",
    "        out[:, :, indices, :] = out_strided\n",
    "        \n",
    "        # Simple nearest neighbor for missing positions\n",
    "        for i in range(N):\n",
    "            if i not in indices:\n",
    "                nearest_idx = indices[torch.argmin(torch.abs(indices - i))]\n",
    "                out[:, :, i, :] = out[:, :, nearest_idx, :]\n",
    "                \n",
    "        return out\n",
    "    \n",
    "    def _fast_linear(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Ultra-fast linear attention\"\"\"\n",
    "        # Normalize for stability\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "        \n",
    "        # Linear attention\n",
    "        kv = k.transpose(-2, -1) @ v\n",
    "        out = q @ kv\n",
    "        \n",
    "        # Simple normalization\n",
    "        out = out / math.sqrt(q.size(-1))\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Benchmark the optimized versions\n",
    "def benchmark_optimized():\n",
    "    \"\"\"Test the optimized Hydra implementations\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"OPTIMIZED HYDRA BENCHMARK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 2\n",
    "    seq_len = 512\n",
    "    embed_dim = 768\n",
    "    num_heads = 12\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, embed_dim, device=device)\n",
    "    \n",
    "    models = {\n",
    "        \"Standard\": nn.MultiheadAttention(embed_dim, num_heads, batch_first=True).to(device),\n",
    "        \"Optimized Hydra\": OptimizedHydraAttention(embed_dim, num_heads, efficiency_mode=\"balanced\").to(device),\n",
    "        \"Ultra-Fast Hydra\": UltraFastHydra(embed_dim, num_heads).to(device),\n",
    "    }\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        \n",
    "        # Parameter count\n",
    "        params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Parameters: {params:,} ({params/1e6:.2f}M)\")\n",
    "        \n",
    "        # Timing\n",
    "        model.eval()\n",
    "        times = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Warmup\n",
    "            for _ in range(5):\n",
    "                if name == \"Standard\":\n",
    "                    _ = model(x, x, x)[0]\n",
    "                else:\n",
    "                    _ = model(x)\n",
    "            \n",
    "            # Actual timing\n",
    "            for _ in range(20):\n",
    "                torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "                start = time.time()\n",
    "                \n",
    "                if name == \"Standard\":\n",
    "                    output = model(x, x, x)[0]\n",
    "                else:\n",
    "                    output = model(x)\n",
    "                    \n",
    "                torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "                times.append(time.time() - start)\n",
    "        \n",
    "        avg_time = np.mean(times[5:]) * 1000  # Convert to ms, skip warmup\n",
    "        std_time = np.std(times[5:]) * 1000\n",
    "        \n",
    "        print(f\"Time: {avg_time:.2f}  {std_time:.2f} ms\")\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        \n",
    "        # Memory\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if name == \"Standard\":\n",
    "                    _ = model(x, x, x)[0]\n",
    "                else:\n",
    "                    _ = model(x)\n",
    "                    \n",
    "            peak_mem = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "            print(f\"Peak Memory: {peak_mem:.3f} GB\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    import numpy as np\n",
    "    benchmark_optimized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c66a2-3d2e-4deb-8a96-d4a7bee79750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECD(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient Channel-Depth attention block - Optimized for reduced FLOPs\n",
    "    \"\"\"\n",
    "    def __init__(self, depth_size: int, hidden_size, proj_size, num_heads=4, qkv_bias=False,\n",
    "                 channel_attn_drop=0.1, spatial_attn_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size//num_heads\n",
    "        \n",
    "        \n",
    "        # Original layers\n",
    "        self.qkv_c = nn.Linear(hidden_size, hidden_size * 3, bias=qkv_bias)\n",
    "        self.qkv_d = nn.Linear(hidden_size, hidden_size, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(channel_attn_drop)\n",
    "        self.attn_drop_2 = nn.Dropout(spatial_attn_drop)\n",
    "        self.out_proj = nn.Linear(hidden_size, int(hidden_size // 2))\n",
    "        self.out_proj2 = nn.Linear(hidden_size, int(hidden_size // 2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        N = H * W\n",
    "        \n",
    "        x_reshaped = x.permute(0, 2, 3, 4, 1).view(B, D, N, C)\n",
    "        \n",
    "        qkv_c = self.qkv_c(x_reshaped).reshape(B, D, N, 3, C).permute(3, 0, 1, 2, 4)\n",
    "        q_d = self.qkv_d(x_reshaped).reshape(B, D, N, C)\n",
    "        q_c, k_c, v_c = qkv_c[0], qkv_c[1], qkv_c[2]\n",
    "        k_shared, v_shared = k_c, v_c\n",
    "        \n",
    "        \n",
    "        # Channel attention \n",
    "        q_c = q_c.reshape(B*D, N, self.num_heads, C//self.num_heads).permute(0, 2, 1, 3)\n",
    "        k_c = k_c.reshape(B*D, N, self.num_heads, C//self.num_heads).permute(0, 2, 1, 3)\n",
    "        v_c = v_c.reshape(B*D, N, self.num_heads, C//self.num_heads).permute(0, 2, 1, 3)\n",
    "        \n",
    "        q_c = q_c.transpose(-2, -1)\n",
    "        k_c = k_c.transpose(-2, -1)\n",
    "        v_c = v_c.transpose(-2, -1)\n",
    "        q_c = F.normalize(q_c, dim=-1)\n",
    "        k_c = F.normalize(k_c, dim=-1)\n",
    "        \n",
    "        attn_CA = (q_c @ k_c.transpose(-2, -1)) / (N ** 0.5)\n",
    "        \n",
    "        attn_CA = attn_CA.softmax(dim=-1)\n",
    "        attn_CA = self.attn_drop(attn_CA)\n",
    "        x_CA = (attn_CA @ v_c).reshape(B, D, C, N).permute(0, 1, 3, 2)\n",
    "        x_CA = self.out_proj(x_CA)\n",
    "        \n",
    "        # Depth attention\n",
    "        q_d = q_d.permute(0, 3, 2, 1).reshape(B*C, N, D) \n",
    "        k_d = k_shared.permute(0, 3, 2, 1).reshape(B*C, N, D) \n",
    "        v_d = v_shared.permute(0, 3, 2, 1).reshape(B*C, N, D) \n",
    "        \n",
    "        q_d = q_d.transpose(-2, -1)\n",
    "        k_d = k_d.transpose(-2, -1)\n",
    "        v_d = v_d.transpose(-2, -1)\n",
    "        q_d = F.normalize(q_d, dim=-1)\n",
    "        k_d = F.normalize(k_d, dim=-1)\n",
    "        \n",
    "        attn_D = (q_d @ k_d.transpose(-2, -1)) / (N ** 0.5)\n",
    "        \n",
    "        \n",
    "        attn_D = attn_D.softmax(dim=-1)\n",
    "        attn_D = self.attn_drop_2(attn_D)\n",
    "        x_D = (attn_D @ v_d)\n",
    "        x_D = x_D.reshape(B, C, D, N).permute(0, 2, 3, 1)\n",
    "        x_D = self.out_proj2(x_D)\n",
    "        \n",
    "        x = torch.cat((x_CA, x_D), dim=-1).permute(0, 3, 1, 2).reshape(B, C, D, H, W)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
